---
title: "Self-similarity in Finance"
author: "Matteo Agate,Alessio Cardarello,Silvia Corsi,Andrea Gulizia"
date: "7/12/2020"

output:  
  pdf_document:
        latex_engine: xelatex
mainfont: Times New Roman
sansfont: Times New Roman
monofont: Times New Roman
fontsize: 12pt

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction
  
The probabilistic description of financial prices, pioneered by Bachelier (1900), initially focused on independent and Gaussian distributed price changes. Studies showed that the empirical distribution of stock market returns is leptokurtic and has heavy tails in comparison with the normal distribution. The in-depth analysis of fractal geometry,developed by Benoit Mandelbrot between 1970 and 1980, led to their application also in financial markets.  He was able to discover the self-similar property of financial markets. Indeed, he first discovered fractals  in  financial  time series, when observing that the same kind of distributions appeared unchanged without characteristic scale. For  this reason,  Mandelbrot  concluded, “The very  heart of finance is a fractal”(Mandelbrot, 2004, 165). The truth is that markets are complex and chaotic systems and their behavior has both systemic and random components.Financial markets are an example of "wild randomness", characterized by concentration and long range dependence.Price changes in financial markets did not follow a Gaussian distribution, but rather Lévy stable distributions having infinite variance. Consequently we will focus on the Fractional Brownian Motion,introduced by Paul Lèvy as a model for price behaviour. So,given these assumptions,we considered it worthwhile to take into account the Fractal Market Hypothesis(FMH) instead of the Efficient Market Hypothesis (EMH).

## Self-similarity

The concept of self-similarity regards many fields,from natural science to social phenomena. A self-similar object is exactly or approximately similar to a part of itself. In mathematics it is also known as expanding symmetry or unfolding symmetry and it is a typical property of fractals. This name was coined by Mandelbrot in 1975 to describe these infinite complex patterns that are self-similar across different scales.  Scale invariance is an exact form of self-similarity where at any magnification (or shrinking) there is a smaller piece of the object that is similar to the whole.   Fractals are created by repeating a simple process over and over in an ongoing feedback loop. The Mandelbrot set is one of the most astonishing discoveries of the human mind. As illustrated in successive zooming, fractals appear the same at different levels. Mandelbrot emphasized the use of fractals as realistic and useful models for describing many "rough" phenomena in the real world.
He was the first to observe self-similarity in economics time series,when he discovered that cotton price time series had approximately the same shape at different time scales. However we are interesting in its implications in the financial area. The self-similarity for stochastic processes can be seen as the invariance in distribution under suitable scaling of time scale. Intuitively, this means that it’s not possible to evaluate the scale of a sample by looking at its distribution. For example,if a price pattern is generated by a process with scaling property, the plots of average daily and monthly prices will appear to be perfectly similar in distribution. As we are considering a random environment, self-similarity applies to distributions, not to the actual realization of a process.
Let $(X,F,P)$ be a probability space,with $t \in [0,\infty)$ being time.
We consider a random process $X= \{X(t,\omega):t \geq 0\}$ where $\omega$ belongs to a sample space $\Omega$.  
Now we can give a proper definition:  
A random process X is said to be self-similar of index H if for any $\alpha>0$ : 
$$\{X(\alpha t,\omega)\} \overset{d}{=} \alpha^H\{X(t,\omega)\} \qquad\qquad\qquad\qquad\qquad\qquad(1)$$   
for all $t\geq 0$ and $0<H<1$ .

A self-similar process is also called uniscaling or unifractal.
If differents values of H are found in various intervals, the process is more likely to be multifractal rather than self-similar.
Multifractality is a form of generalized scaling that includes both extreme variations and long-memory.
Unfortunately this is not the topic of our research. 
A second definition of self-similarity, more appropriate in the context of standard time series theory, involves a stationary sequence.
Let $\{X(j) \land j=0,1,..N\}$ denote the discrete sequence of increments of a stationary and self-similar process. 
In recent financial time series analysis, long memory processes are one of the most important elements in the informationally efficient market and risk management analyses. The long memory property indicated predictability components to some extent against the efficiency market hypothesis. The question that spontaneously arises is:how can we investigate the presence of long term memory? The answer is given to us by the concept of Hurst exponent. The Hurst exponent provides a way to measure the amount by which a financial time series deviates from a random walk. The name "Hurst exponent" derives from Harold Edwin Hurst (1880–1978) and his studies on the 
long-term storage capacity of reservoirs. He conducted an analysis to determine the optimum dam sizing for the Nile River’s volatile rain and drought conditions observed over a long period of time. In doing so, he developed the empirical rescaled range methodology for measuring long-range dependence. The Hurst exponent can assume values between 0 and 1.

Based on this, a time series can be classified into three categories:

1. $H=\frac{1}{2}$ indicates a random series:we have a martingale\footnote{A stochastic process is a martingale if the conditional expectation of the next value in the sequence, regardless of all prior values, is equal to the present value} .

2. $0<H<\frac{1}{2}$ indicates an anti-persistent series which has a characteristic of mean-reverting.
The strength of “meanreverting” increases as $H$ $\to$ 0.

3. $\frac{1}{2}<H<1$ indicates a persistent series,so the process is characterized by a long-term memory.The strength of the trend increases as $H$ $\to$ 1 .  
Most economic and financial time series are persistent with $H>$$1\over2$.

![half-size image](C:/Users/ale-r/Desktop/PRIMO SEMESTRE/QUANTITATIVE FINANCIAL MODELLING/PROGETTO/PROGETTO BIANCHI/HUNITI.png){width=100%}

## Self-similarity in finance
 
In finance there are many different models that take into account the self similarity. Among the others, we can mention the $\alpha$-stable Lèvy motion, the class of Hermite processes and the two that we are going to analyze: the Standard Brownian Motion and the Fractional Brownian motion. The essential difference between the latters is the value of the parameter that describes the self similarity. In fact the Standard Brownian Motion assumes $H=\frac{1}{2}$, meanwhile in the Fractional Brownian motion the exponent $H\in[0,1)$. This means that the first model assumes the randomness of the price process, meanwhile the other implies the possibility of time path inside the process. This assumption is the fundamental difference between the two most important market hypothesis: the Efficient Market Hypothesis and the Fractal Market Hypothesis.  A capital market is said to be efficient if it fully and correctly reflects all relevant information in determining security prices. Formally, the market is said to be efficient with respect to some information set, $F_t$, if security prices would be unaffected by revealing that information to all participants. Moreover, efficiency with respect to an information set, $F_t$, implies that it is impossible to make economic profits by trading on the basis of $F_t$ (Fama,1970). Some periods, better than others, show the difficulty of this theory to describe the reality of the financial market. If the prices were fully determined by the new information, huge shocks...??. Obviously this model implies the non correlation of the increments since there is no presence of long memory. The investigation of a long term memory in time series and particularly in financial time series has always had a crucial importance in the estimation, forecasting and especially in the market risk determination. The reason of this importance lies in the fact that through the discovery of the presence of long term memory we are able to recognize a recurring path in the time series and so there is the possibility, on certain level, to make a prediction of the future. The existence of the market memory could implicate the rejection of the efficient market hypothesis. The Fractal Market Hypothesis instead take into account the presence of long memory and so the dependence of the increments of the process. Weron et al. describe perfectly the properties of the Fractal Market Hypothesis. According to them, they are:

* The existence of different investment horizons in the market, depending on the expectations of the investors;
* information has a different impact on these different investment horizons; 
* liquidity is determined by the presence of investors with heterogeneous investment horizons; 
* the short-term investor is focused on daily prices changes, the long-term investors is concerned about the fundamentals. The combination of these different expectations determines market prices;
* if there is no balance of short-term and long-term expectations, short-term expectations will prevail.

In this context it can be used the information that ere in the past prices to do forecast, and this practice is called technical analysis.
Two of the most common trading strategies used by investors are the momentum and mean reversion strategies. The first one is a trading strategy in which investors buy securities that are rising and sell them when they look to have peaked. The goal is to work with volatility by finding buying opportunities in short-term uptrends and then sell when the securities start to lose momentum. The second one suggests that asset prices and historical returns eventually will revert to the long-run mean or average level of the entire dataset. In both regimes, the current price contains useful information about the future price. In fact, trading strategies can only generate profit if asset prices are either trending or mean-reverting since, otherwise, prices are following what is known as a random walk. 

Another important link beetween self similarity and real financial market is given by Bianchi et al. (2020). In their work "A distribution-based method to gauge market liquidity through scale invariance between investment horizons" they link the self similarity parameter with the liquidity of the market. Liquidity is defined by Nicola Anderson as the ease with which an investor is able to buy/sell a security without their act of buying/selling having a substantial effect on its price. This liquidity, under the Fractal Market Hypothesis is ensured by the fact that the investors with different investment horizons treat the information differently (e.g. for a daily trader a given information could be interpreted as a signal of selling a stock and the same information could represent a signal of buying a stock for a yearly investor). Bianchi et al. developed a self similarity estimator based on the entire distribution for any pair of time horizon, instead of a parameter based on a single moment of the distribution. In order to understand the relationship which link this estimator with the liquidity of the fraction of the market operating at time scales a and b they showed a clarificator example: it has been reproduced in this work with some differences. 
Assume the log return of a stock is distributed as a Normal with zero mean and $H=\frac{1}{2}$. Consider only two time scales: $a=1$ day and $b=6$ days (a trading week) . Assume also that the day volatility is $\sigma_1=1$. The 6 days volatility can be calculated as $\sigma_{6}=6^{0.5}\sigma_{1}=2.45$. The weekly distribution is fatter than the one day distribution (see Figure 2) and its amplitude grows if the self similarity parameter grows. A negative shock on price of a stock ($-2\sigma_1$) occurs. Projecting this value on the distribution of Figure 2 it's possible to calculate the conditional probability that the successive log return will be negative. 
$$
\phi_{Y(t,1)}(-2)=0.022=2.2\%
$$
$$
\phi_{Y(t,6)}(-2)=0.207=20.7\% \ for \ H=0.5
$$
$$
\phi_{Y(t,6)}(-2)=0.316=31.6\% \ for \ H= 0.8 
$$
$$
\phi_{Y(t,6)}(-2)=0.099=9.9\% \ for \ H= 0.25   
$$ 
![Figure 2](C:/Users/ale-r/Desktop/PRIMO SEMESTRE/QUANTITATIVE FINANCIAL MODELLING/PROGETTO/PROGETTO R PART/DISTRIBUTION.png){width=100%}
For a daily investor, the probability of a negative log return is very low, 2.2%, so probably he will buy that stock. For a weekly investor, it's clear that if H grows, the probability to have a successive negative log return increases, so there will be more weekly investors which choose to sell that stock which match the long position of the daily investors. If the H is very low, daily and weekly investors operate in the same direction (in this case the long position) and there will be a problem of liquidity in the time scale (1,6).


## Estimation of Hurst Exponent

There are many techniques for the estimation of the Hurst parameters and they are based on different methodologies. A family of those is based on the escalation of different moment of the distribution and works in the time domain: the Range Scaled Analysis, the Absolute Average Value, the Higuchi method, the Absolute Variance and the Detendred Fluctuation Analysis belongs to this family.
Another family of techniques is based on the geometrical calculation and interpretation of the Hurst Exponent. In this work two types of these techniques are discussed. 
Finally, a distribution based method (Bianchi) is briefly introduced.

### Moments based family

Speaking about this family of methodologies, there are two step that are common, except for Higuchi method.
Since financial times series are not stationary, the first step of the procedures is the calculation of the returns of the series, that is the difference of the logarithm of X:

$$
X_{t}=log(Y_{t})-log(Y_{t-1})=log \left(\frac{Y_{t}}{Y_{t-1}}\right)
$$
The second common step is the division of the obtained log returns series. For doing that the division proposed by Taqqu and Teverovsky, that is:

Divide the discrete time series $\{X(i)\}$ of length $N$ into $N/m$ non overlapping
blocks of length $m$ and consider the sequence $X_m(k)$ of the blocks’ sample averages:  
$$X_m(k)= {1\over m} \sum_{t=(k-1)m}^{km} X(t) \quad  k=1,2,...[N/m] \qquad\qquad (2)$$
\textit{a) R/S analysis}

The R/S method was invented by Hursts and its spread was assisted by Mandelbrot and Wallis.Once the the time series has been splitted, the mean and the standard deviation of each block are calculated. Now the mean of the block  has to be subtracted from every values of each block .
$$
N_j=X_j-E_i
$$
Then it has to be calculated the cumulative of the series
$$
Z_{i,m}=  \sum_{j=1}^{i}N_{j,m}
$$
The range is defined has the difference between the higher value and the lowest one:
$$
R_i= max(Z_i)-min(Z_i)
$$
By dividing the range by the standard deviation of the block, the range scaled is calculated:
$$
RS_m = \frac{i}{d}\sum_{i=1}^{d}R_i/S_i
$$
In order to calculate the Hurst parameter, we have to estimate a the coefficient of the linear regressor between the logarithm of the Rescaled range and the logarithm of $m$.

\textit {b) Absolute Value Method}

The second method for the estimation of the Hurst parameter is based on the mean of the block in  which we divide the series.
So the first step of the Average Value method is the same of the rescaled range analysis, that is the calculation of the returns of the series and the division of it into $d$ block of lenght $n$.
The second step is the calculation of the mean $M_i$ of each block $X_i={X_1,...X_m}$ for all $i=1,..,d$.
Then it has to be calculate the mean of the absolute values of $E_i$
Since the estimated value for the slope of the regression model of the logarithm of the absolute mean and the log of $n$ is equal to $H+1$, the value of the coefficient will be b-1. This method works appropriately for the financial time series with high volatility.

\textit {c) Aggregated Variance method}

The Aggregated Variance method does not deal with the first moment of the time series as the previous one. Instead it deals with the second centered moment, i.e. the variance of each block. In fact the method is based by the fact that the variance of the block-average sequence is asymptotically distributed as
$$
 Var(X_m)=m^{(2H - 2)} \sigma^2
$$

Practically, it means that we have to take the mean of all the variances of the blocks, and then fit a linear regression between the logarithm of the latter to the logarithm of m. In order to have the value of the exponent, we have to divide the coefficients of the slope by 2 and add 1/2.

$$
log(Var(X_m))= {(2H - 2 )}log(m)
$$
So the value of the Hurst exponent is derived by 1 plus the coefficient of the regression.


\textit {d) Detrended fluctuation analysis}

The procedure of the detrended fluctuation analysis is similar to the Range scaled, the difference is that the series has to be detrended. After the usual calculation of the log returns and the division in block, it has to be calculated the cumulative series:
$$
X_{i,m}=  \sum_{j=1}^{i}Y_{j,m}
$$
This means that it is necessary the calculation of a linear regression between each block and the series that goes from 1 to the length of the correspondent block.
$$
\hat{X}_m(t)= \alpha_m t +b_m
$$
Each values of each block has to be subtracted by the coefficients of the linear regression. In such a way the series is said to be "detrended". Then, it has to be calculated the root mean square fluctuation of the series obtained
$$
F(m)=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(X_{i,m}-\alpha_m(i)-b_m)^2}
$$
The last step before the estimation of the linear regression that allows us to calculate the Hurst exponent is the calculation of the mean value of the root mean square for all the subset of length n.
$$
F(n)= \frac{1}{d}\sum_{m=1}^{d}F(m)
$$
The  value of $H$ is the estimation of the slope between the logarithm of the series obtained and the logarithm of $n$.

\textit {e) Higuchi Method}

Higuchi designed a method for finding the fractal dimension of an irregular curve, which is given by the following algorithm. Consider a finite set of time series observations taken at regular intervals:
$$
X(1),X(2),...,X(N)
$$
From this time series, are generated new time series:
$$
X^m_k:X(m),X(m+k),X(m+2k),...,X(m+[\frac{N-m}{k}]k)
$$
with $m=1,...,k$

Both $m$ and $k$ are integers and [] denotes Gauss' notation. $k$ and $m$ represent the interval time and initial time, respectively. For every time interval $k$, a total of $k$ new sets of time series are obtained. “The length” of the curve $L_m(k)$ is computed for each of the $k$ time series or curves $X^m_k$.
$$
L_m(k)=\frac{1}{k} \left[\left(\sum_{i=1}^{int[\frac{N-m}{k}]}|x(m(ik)-x(m+(i-1)k)|\right)\frac{N-1}{int\left[\frac{N-m}{k}\right]k}\right]
$$
where $N$ is the length of the original time series $X$. The following step is the averaging of $L_m(k)$ for all $m$: the result represents the mean value of the curve length $L(k)$ for each $k=1,...,k_{max}$ as
$$
L(k)=\frac{\sum_{m=1}^kL_m(k)}{k}
$$
$k_{max}$ values $L(k)$ are obtained. In Higuchi’s algorithm there is a need to choose the value for $k_{max}$. But the criterion for the selection of these values is not presented, so it gives less effective results. Higuchi fixed the minimum value of $K=2$ and maximum value of $K=8$ for his algorithm. The Higuchi's Fractal Dimension is estimated as the slope of least squarest linear best fit from the plot of $log(L(k))$ with $log(1/k)$. In order to find the Hurst Exponent $H$:
$$
H=2-HFD
$$

### Geometric method family

Another different approach for the estimation of the Hurst Exponent is the one adopted by the Geometric method-based procedures. The first difference is the composition the fact that this method operates with the logarithm of the price series instead of the log return. 

*GM1 algorithm*

For a given time series of log prices of length n, and for each $m=2^k<n$, the division into $d=\frac{n}{m}$ non overlapping blocks of length $m$. Then

* calculate the variation of each block $\mathcal{B_i}=[{B_1,...B_m}]$, namely $D_i = B_m-B_1$ for $i=1,...,d$

* calculate the mean of the variations of all the blocks $M_m=\frac{1}{d}\sum_{i=1}^dD_i$

The value of $H_{GM1}$ is the slope of the linear regression of log $M_m$ with log $m$

*b) GM2 algorithm*

The GM2 algorithm is different respect to GM1 because instead of the variation of each block is defined the range of the blocks $R_i=max[B_j:j=1,...,m] - min[B_j:j=1,...,m]$ for $i=1,...,d$.
After that the mean of the ranges of the blocks has been calculated: $M_m=\frac{1}{d}\sum_{i=1}^dR_i$
The value of $H_{GM2}$ is the slope of the linear regression of log $M_m$ with log $m$.





