---
output:  
  pdf_document:
        latex_engine: xelatex
mainfont: Times New Roman
sansfont: Times New Roman
monofont: Times New Roman
fontsize: 12pt
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
```



## Introduction

The probabilistic description of financial prices, pioneered by Bachelier (1900), initially focused on independent and Gaussian distributed price changes. Studies showed that the empirical distribution of stock market returns is leptokurtic and has heavy tails in comparison with the normal distribution. The in-depth analysis of fractal geometry, developed by Benoit Mandelbrot between 1970 and 1980, led to their application also in financial markets.  He was able to discover the self-similar property of financial markets. Indeed, he first discovered fractals  in  financial  time series, when observing that the same kind of distributions appeared unchanged without characteristic scale. For  this reason,  Mandelbrot  concluded: “The very  heart of finance is a fractal” (Mandelbrot, 2004). The truth is that markets are complex and chaotic systems and their behavior has both systemic and random components. Financial markets are an example of "wild randomness", characterized by concentration and long range dependence. Prices changes in financial markets do not follow a Gaussian distribution, but rather Lévy stable distributions having infinite variance. Consequently we will use the Fractional Brownian Motion, introduced by Paul Lèvy as a model for price behaviour. So, given these assumptions, we considered it worthwhile to take into account the Fractal Market Hypothesis (FMH) instead of the Efficient Market Hypothesis (EMH).

## Self-similarity

The concept of self-similarity regards many fields ,from natural science to social phenomena. A self-similar object is exactly or approximately similar to a part of itself. In mathematics it is also known as expanding symmetry or unfolding symmetry and it is a typical property of fractals. This name was coined by Mandelbrot in 1975 to describe these infinite complex patterns that are self-similar across different scales.  Scale invariance is an exact form of self-similarity where at any magnification (or shrinking) there is a smaller piece of the object that is similar to the whole. Fractals can be oserved in many natural and human phenomena, as clouds, blood vessels ramifications and coastlines. In mathematics fractals could be obtained by repeating a simple process over and over in an ongoing feedback loop. Mandelbrot emphasized the use of fractals as realistic and useful models for describing many "rough" phenomena in the real world.
He was the first to observe self-similarity in economics time series, when he discovered that cotton price time series had approximately the same shape at different time scales. However we are interested in its implications in the financial area. The self-similarity for stochastic processes can be seen as the invariance in distribution under suitable scaling of time scale. Intuitively, this means that it is not possible to evaluate the scale of a sample by looking at its distribution. For example, if a price pattern is generated by a process with scaling property, the plots of average daily and monthly prices will appear to be similar in distribution. As we are considering a random environment, self-similarity applies to distributions, not to the actual realization of a process.  
Let $(X,F,P)$ be a probability space,with $t \in [0,\infty)$ being time. We consider a random process $X= \{X(t,\omega):t \geq 0\}$ where $\omega$ belongs to a sample space $\Omega$. A random process X is said to be self-similar of index H if for any $\alpha>0$ : 
$$\{X(\alpha t,\omega)\} \overset{d}{=} \alpha^H\{X(t,\omega)\} )$$ 
for all $t\geq 0$ and $0<H<1$ . 

A self-similar process is also called uniscaling or unifractal.
If differents values of H are found in various intervals, the process is more likely to be multifractal rather than self-similar.
Multifractality is a form of generalized scaling that includes both extreme variations and long-memory. A second definition of self-similarity, more appropriate in the context of standard time series theory, involves a stationary sequence.  
Let $\{X(j)$ and $j=0,1,..N\}$ denotes the discrete sequence of increments of a stationary and self-similar process. 
In recent financial time series analysis, long memory processes are one of the most important elements in the informationally efficient market and risk management analyses. The long memory property indicates predictability components to some extent against the efficiency market hypothesis. The question that spontaneously arises is: how can we investigate the presence of long term memory? The answer is given to us by the concept of Hurst exponent. It provides a way to measure the amount by which a financial time series deviates from a random walk. The name "Hurst exponent" derives from Harold Edwin Hurst (1880–1978) and his studies on the 
long-term storage capacity of reservoirs. He conducted an analysis to determine the optimum dam sizing for the Nile River’s volatile rain and drought conditions observed over a long period of time. In doing so, he developed the empirical rescaled range methodology for measuring long-range dependence. The Hurst exponent can assume values between 0 and 1.  
Based on this, a time series can be classified into three categories: 

1. $H=\frac{1}{2}$ indicates a random series: we have a martingale\footnote{A stochastic process is a martingale if the conditional expectation of the next value in the sequence, regardless of all prior values, is equal to the present value.};

2. $0<H<\frac{1}{2}$ indicates an anti-persistent series which has a characteristic of mean-reverting.
The strength of “mean-reverting” increases as $H$ $\to$ 0;

3. $\frac{1}{2}<H<1$ indicates a persistent series,so the process is characterized by a long-term memory.The strength of the trend increases as $H$ $\to$ 1 .  
Most economic and financial time series are persistent with $H>$$1\over2$.

![Dow Jones, own elaboration. Left plot: anti-persistent (1991); Center plot: random walk (1996); Right plot: persistent(Q4 2016-Q3 2017). ](C:/Users/ale-r/Desktop/PRIMO SEMESTRE/QUANTITATIVE FINANCIAL MODELLING/PROGETTO/PROGETTO BIANCHI/HUNITI.png){width=100%}

In Figure 1 we can see time series plots with different Hurst values. In the plot on the left there is not an evident trend and we can underline the presence of the mean-reverting property by the frequent ups and downs of the prices. The central plot represents a random walk. The plot on the right shows a persistent series, in fact it is present a positive trend. As the Hurst exponent increases the series is smoother.

## Self-similarity in finance

In finance there are many different models that take into account the self similarity. Among the others, we can mention the $\alpha$-stable Lèvy motion, the class of Hermite processes and the two that we are going to analyze: the Standard Brownian Motion and the Fractional Brownian motion. The essential difference between them is the value of the parameter that describes the self similarity. In fact the Standard Brownian Motion assumes $H=\frac{1}{2}$, meanwhile in the Fractional Brownian motion the exponent $H\in[0,1)$. This means that the first model assumes the randomness of the price process, meanwhile the other implies the possibility of time path inside the process. This assumption is the fundamental difference between the two most important market hypothesis: the Efficient Market Hypothesis and the Fractal Market Hypothesis.  A capital market is said to be efficient if it fully and correctly reflects all relevant information in determining security prices. Formally, "the market is said to be efficient with respect to some information set, $F_t$, if security prices would be unaffected by revealing that information to all participants. Moreover, efficiency with respect to an information set, $F_t$, implies that it is impossible to make economic profits by trading on the basis of $F_t$" (Fama,1970). Some periods, better than others, show the difficulty of this theory to describe the reality of the financial market. The EMH implies the non correlation of the increments since there is no presence of long memory. The investigation of a long term memory in time series and particularly in financial time series has always had a crucial importance in the estimation, forecasting and especially in the market risk determination. The reason of this importance lies in the fact that through the discovery of the presence of long term memory we are able to recognize a recurring path in the time series and so there is the possibility, on certain level, to make a prediction of the future. The existence of the market memory could implicate the rejection of the efficient market hypothesis. Instead, the Fractal Market Hypothesis take into account the presence of long memory and so the dependence of the increments of the process. Weron et al. describe perfectly the properties of the Fractal Market Hypothesis. According to them, they are:

* The existence of different investment horizons in the market, depending on the expectations of the investors.
* Information has a different impact on different investment horizons. 
* Liquidity is determined by the presence of investors with heterogeneous investment horizons. 
* The short-term investor is focused on daily prices changes, the long-term investor is concerned about the fundamentals. The combination of these different expectations determines market prices.
* If there is no balance of short-term and long-term expectations, short-term expectations will prevail.

In this context it can be used the information included in past prices to do forecast, and this practice is called technical analysis.
Two of the most common trading strategies used by investors are the momentum and mean reversion strategies. The first one is a trading strategy in which investors buy securities that are rising and sell them when they look to have peaked. The goal is to work with volatility in order to find buying opportunities in short-term uptrends; then sell when the securities start to lose momentum. The second one suggests that asset prices and historical returns eventually will revert to the long-run mean or average level of the entire dataset. In both regimes, the current price contains useful information about the future price. In fact, trading strategies can only generate profit if asset prices are either trending or mean-reverting since, otherwise, prices are following what is known as a random walk.  
Another important link between self similarity and real financial market is given by Bianchi et al. (2020). In their work they link the self similarity parameter with the liquidity of the market. Market liquidity is defined by Nicola Anderson (2013) as the "relative ease with
which an investor is able to buy/sell a security without their
act of buying/selling having a substantial effect on its price". This liquidity, under the Fractal Market Hypothesis, is ensured by the fact that the investors with different investment horizons treat the information differently (e.g. for a daily trader a given information could be interpreted as a signal of selling a stock and the same information could represent a signal of buying a stock for a yearly investor). Bianchi et al. developed a self similarity estimator based on the entire distribution for any pair of time horizons, instead of a parameter based on a single moment of the distribution. About the link between this estimator and the market liquidity fraction, operating in a time scale (*a*, *b*), they showed a clarifying example: assume the log return of a stock is distributed as a Normal with zero mean and $H=\frac{1}{2}$. Consider only two time scales: $a=1$ day and $b=5$ days (a trading week) and assume the day volatility is $\sigma_1=1$. The 5 days volatility can be calculated as $\sigma_{5}=5^{0.5}\sigma_{1}=2.45$. The weekly distribution is fatter than the one day distribution (see Figure 2) and its amplitude grows if the self similarity parameter grows. A negative shock on price of a stock ($-2\sigma_1$) occurs. Projecting this value on the distributions of Figure 2 it is possible to calculate the conditional probability that the successive log return will be negative. 
$$
\phi_{Y(t,1)}(-2)=0.022=2.2\%
$$
$$
\phi_{Y(t,5)}(-2)=0.183=18.3\% \ for \ H=0.5
$$
$$
\phi_{Y(t,5)}(-2)=0.289=28.9\% \ for \ H= 0.8 
$$
$$
\phi_{Y(t,5)}(-2)=0.090=9\% \ for \ H= 0.25   
$$ 

For a daily investor, the probability of a negative log return is very low, 2.2%, so probably he will buy that stock. For a weekly investor, it is clear that if H grows, the probability to have a successive negative log return increases, so there will be more weekly investors who choose to sell that stock which match the long position of the daily investors. If the H is very low, daily and weekly investors operate in the same direction (in this case the long position) and there will be a problem of liquidity in the time scale (1,5).

![Four Normal distribution functions with zero mean and variance dependent on time scale and Hurst exponent](C:/Users/ale-r/Desktop/PRIMO SEMESTRE/QUANTITATIVE FINANCIAL MODELLING/PROGETTO/PROGETTO R PART/Rplot.png){width=75%}


## Estimation of Hurst Exponent

There are many techniques for the estimation of the Hurst parameter and they are based on different methodologies. A kind of those takes into account different moments of the distribution and works in the time domain: the Range Scaled Analysis, the Absolute Average Value, the Higuchi method, the Absolute Variance and the Detendred Fluctuation Analysis.
Another family of techniques is based on the geometrical calculation and interpretation of the Hurst exponent. Finally, a distribution based method is briefly introduced.  

The moment based methodologies share the same initial steps. The non stationarity of financial series implies the calculation of the series returns, so the difference of the logarithm of X. We compute the division of the obtained log returns series, applying the Taqqu and Teverovsky theorem:

Divide the discrete time series $\{X(i)\}$ of length $N$ into $N/m$ non overlapping
blocks of length $m$ and consider the sequence $X_m(k)$ of the blocks’ sample averages:  
$$X_m(k)= {1\over m} \sum_{t=(k-1)m}^{km} X(t) \quad  k=1,2,...[N/m]$$
\textit{a) R/S analysis}

The R/S method was invented by Hurst. Once the time series has been splitted, the mean and the standard deviation of each block are calculated. Then the mean of the block, $E_i$, has to be subtracted from each values $X_j$ of the block.
$$
N_j=X_j-E_i
$$
Then the cumulative of the series has to be calculated 
$$
Z_{i,m}=  \sum_{j=1}^{i}N_{j,m}
$$
The range is defined as the difference between the higher value and the lowest one:
$$
R_i= max(Z_i)-min(Z_i)
$$
By dividing the range by the standard deviation of the block, the range scaled is calculated:
$$
RS_m = \frac{i}{d}\sum_{i=1}^{d}R_i/S_i
$$
In order to calculate the Hurst parameter, we have to estimate the coefficient of the linear regressor between the logarithm of the Rescaled range $RS_m$ and the logarithm of $m$.

\textit {b) Absolute Value Method}

The first step is the calculation of the mean $M_i$ of each block $X_i={X_1,...X_m}$ for all $i=1,..,d$.
The mean of the absolute values of $E_i$ has to be calculated.  
We obtain $H$ value from the slope $\hat{\beta}$ of the regression model between the logarithm of the absolute mean and the log of n:
$$H=\hat{\beta}-1$$
 This method works appropriately for the financial time series with high volatility.

\textit {c) Aggregated Variance method}

The Aggregated Variance method does not deal with the first moment of the time series as the previous one. Instead, it deals with the second centered moment, i.e. the variance of each block. The method is based on the variance of the block-average sequence, that is asymptotically distributed as
$$
 Var(X_m)=m^{(2H - 2)} \sigma^2
$$

Practically, it means that we have to take the mean of all the variances of the blocks. Then fit a linear regression:

$$
log(Var(X_m))= {(2H - 2 )}log(m)
$$
In order to obtain the Hurst exponent:
$$
H=\frac{\hat{\beta}}{2}+\frac{1}{2}
$$
\newpage
\textit {d) Detrended fluctuation analysis}

The procedure of the detrended fluctuation analysis is similar to the Range scaled, the difference is that the series has to be detrended. After the usual calculation of the log returns and the division in blocks, it has to be calculated the cumulative series:
$$
X_{i,m}=  \sum_{j=1}^{i}Y_{j,m}
$$
In order to remove the trend it is necessary the calculation of a linear regression between each block and the series that goes from 1 to the length of the correspondent block.
$$
\hat{X}_m(t)= \alpha_m t +b_m
$$
Each value of each block has to be subtracted by the coefficients of the linear regression. In such a way the series is said to be "detrended". Then,  the root mean square fluctuation of the obtained series has to be calculated.
$$
F(m)=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(X_{i,m}-\alpha_m(i)-b_m)^2}
$$
The last step before the estimation of the linear regression that allows us to calculate the Hurst exponent is the calculation of the mean value of the root mean square, for all the subset of length n.
$$
F(n)= \frac{1}{d}\sum_{m=1}^{d}F(m)
$$
The  value of $H$ is the estimation of the slope between the logarithm of the obtained series and the logarithm of $n$.

\textit {e) Higuchi Method}

Higuchi designed a method to find the fractal dimension of an irregular curve, which is given by the following algorithm. Consider a finite set of time series observations taken at regular intervals:
$$
X(1),X(2),...,X(N)
$$
From this time series, new time series are generated:
$$
X^m_k:X(m),X(m+k),X(m+2k),...,X(m+[\frac{N-m}{k}]k)
$$
with $m=1,...,k$

Both $m$ and $k$ are integers and denotes Gauss' notation. $k$ and $m$ represent the interval time and initial time, respectively. For every time interval $k$, a total of $k$ new sets of time series are obtained. “The length” of the curve $L_m(k)$ is computed for each of the $k$ time series of curves $X^m_k$.
$$
L_m(k)=\frac{1}{k} \left[\left(\sum_{i=1}^{int[\frac{N-m}{k}]}|x(m(ik)-x(m+(i-1)k)|\right)\frac{N-1}{int\left[\frac{N-m}{k}\right]k}\right]
$$
where $N$ is the length of the original time series $X$. The following step is the average of $L_m(k)$ for all $m$: the result represents the mean value of the curve length $L(k)$ for each $k=1,...,k_{max}$ as $k_{max}$ values $L(k)$ are obtained. 
$$
L(k)=\frac{\sum_{m=1}^kL_m(k)}{k}
$$
The Higuchi's Fractal Dimension $(HFD)$ is estimated as the slope of the regression of $log(L(k))$ with $log(1/k)$. In order to find the Hurst Exponent $H$:
$$
H=2-HFD
$$

\textit {f) Geometric based method}

In this work we analyze two geometric based method procedures: GM1 and GM2. Geometric procedures differ from the others in the application of the logarithm of the price series instead of the log return.
Given a time series of log prices of length n, and for each $m=2^k<n$, a division into $d=n/m$ non overlapping blocks of length $m$ is done. Then we calculate the variation of each block $\mathcal{B_i}=[{B_1,...B_m}]$, namely $D_i = B_m-B_1$ for $i=1,...,d$.
The last step is the evaluation of the mean of the variations of all the blocks $M_m=\frac{1}{d}\sum_{i=1}^dD_i$.  

The value of $H_{GM1}$ is the slope of the linear regression of log $M_m$ with log $m$.
The difference between GM1 and GM2 is in the calculation of the range value. In fact, instead of the variation of each block the method considers the maximum and the minimum $R_i=max[B_j:j=1,...,m] - min[B_j:j=1,...,m]$.

\textit {g) Distribution based method}

In this section a distribution-method based (Bianchi) is described.
The intuition is that scaling the entire distribution, instead of moments, better represents the self similarity property. In fact, studying only the scale of one moment can be misleading, because it could not describe the entire phenomenon. 
Considering a stochastic process we would able to calculate only one trajectory $X_t=ln(T_t) \land t=1,...,T$, but in this way, with a fixed time t, no distributions could be obtained. To avoid that, we fix a time window of length $\Delta -s-1$, starting from time t. For any pair of time scales (a, b), we calculate $^\alpha \hat{H}(a, b, t^{0}_{\Delta})$ as:$$ \hat{H}^\alpha(a,b,t_\Delta^0)=arg min\hat{\delta}(\Psi_H) $$ Where $\delta(\Psi_{H})$ is the diameter of the metric space : $$\delta(\Psi_H)= max |\hat{\Phi}_{Y(t_\Delta^a,a)}(x)-\hat{\Phi}_{Y(t_\Delta^b,b)}((b/a)^Hx)| $$ The process is self-similar if $\delta(\Psi_{H}) = 0$. To test this preposition, we use the Kolmogorv-Smirnov statistic. Considering the total time window, we can obtain the sequence of the $\alpha$ self-similarity matrices $^\alpha \hat{H}t^{0}_{\Delta}$.

\newpage
### Conclusions
In this work the self-similarity in finance has been studied. The starting point was the Mandelbrot contributions and its definition of the concept. The Hurst exponent, the key element of the self-similarity, is useful to detect a time series. Fractals are used to describe stock prices fluctuation, since their properties better capture market behaviour. When the Hurst value is different from ½, the market does not follow the Efficient Market Hypothesis. Fractal Market Hypothesis is one of the most effective frameworks. Persistent financial series are interesting for an investor who looks for long-term opportunities. A link with the market liquidity has been highlighted. It has been provided a description of some techniques to calculate the Hurst parameter. The self-similarity is a huge topic in finance, not new, but not completely exhausted in the details. We tried to make order, focusing our attention on the self-similarity and the estimation of the Hurst exponent.
